{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3308faa-0708-424e-820d-8d1d8545df72",
   "metadata": {},
   "source": [
    "# Batch1-Coding Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92be6-ea8b-42fe-9519-e1ccebbbe149",
   "metadata": {},
   "source": [
    "# Question 1: Image Preprocessing for Inference (PyTorch)\n",
    "# Write a function to load an image and preprocess it for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f854aded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cpu\n",
      "0.25.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c78270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class ID: 985\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess image\n",
    "def preprocess_image(image_path):\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Load image\n",
    "input_image = preprocess_image(\n",
    "    r\"C:\\Balaji\\Balaji AI\\New folder\\Week11-Deep Learning Module\\Deep Learning (Week 11)\\Batch1\\1.Deep_Learning_Question_Set1\\2.Debugging Error\\Images_to_use\\flower.jpg\"\n",
    ")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    predicted_class = output.argmax(1).item()\n",
    "\n",
    "# Print result\n",
    "print(\"Predicted Class ID:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5e91b9-5b55-4d32-afe3-c7a03bf0a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1f1d8-0dd7-441e-a4a5-63a8d6230125",
   "metadata": {},
   "source": [
    "# Question 2: Predict on New Image with a Trained Model\n",
    "# Perform prediction and get the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758a9ad2-734f-4339-acc1-7d7122006a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 985\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_image = preprocess_image(\n",
    "    r\"C:\\Balaji\\Balaji AI\\New folder\\Week11-Deep Learning Module\\Deep Learning (Week 11)\\Batch1\\1.Deep_Learning_Question_Set1\\2.Debugging Error\\Images_to_use\\flower.jpg\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    predicted_class = output.argmax(1).item()\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1728a-405d-421f-9d21-9a4d5620b0f8",
   "metadata": {},
   "source": [
    "# Question 3: Build a CNN to classify CIFAR-10 images (PyTorch)\n",
    "# Create a CNN model that classifies images from the CIFAR-10 dataset with accuracy above 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb9b03d-a48a-462c-9b26-b5f8a13cac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [10:21<00:00, 274kB/s]    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*8*8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training (1 epoch shown for simplicity)\n",
    "for epoch in range(1):\n",
    "    for images, labels in trainloader:\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6935eb8-e2a9-492d-95f0-e1dc58506aef",
   "metadata": {},
   "source": [
    "# Question 4: Identify Overfitting from Training Logs and Solve It\n",
    "# Problem: You notice the training accuracy increases but validation accuracy stagnates. Modify the model using dropout and early stopping(use mnist dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "013896dd-4f01-4140-a1ee-21478c7c193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\good4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 12ms/step - accuracy: 0.9111 - loss: 0.2975 - val_accuracy: 0.9646 - val_loss: 0.1118\n",
      "Epoch 2/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9566 - loss: 0.1464 - val_accuracy: 0.9722 - val_loss: 0.0899\n",
      "Epoch 3/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9650 - loss: 0.1125 - val_accuracy: 0.9758 - val_loss: 0.0869\n",
      "Epoch 4/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.9703 - loss: 0.0992 - val_accuracy: 0.9777 - val_loss: 0.0742\n",
      "Epoch 5/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9736 - loss: 0.0855 - val_accuracy: 0.9766 - val_loss: 0.0752\n",
      "Epoch 6/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.0792 - val_accuracy: 0.9813 - val_loss: 0.0652\n",
      "Epoch 7/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9769 - loss: 0.0733 - val_accuracy: 0.9812 - val_loss: 0.0693\n",
      "Epoch 8/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9790 - loss: 0.0675 - val_accuracy: 0.9809 - val_loss: 0.0721\n",
      "Epoch 9/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.9805 - loss: 0.0610 - val_accuracy: 0.9817 - val_loss: 0.0728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d179fb9eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9947878-50d4-44ae-a254-670898589add",
   "metadata": {},
   "source": [
    "# Question 5: Transfer Learning with Pretrained VGG16 (Cats vs Dogs)\n",
    "# Problem: Use VGG16 for binary classification with fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189ad90e-8648-4630-992b-d5fad8a5d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "base_model = VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
